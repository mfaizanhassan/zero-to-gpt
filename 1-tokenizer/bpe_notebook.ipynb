{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ef4b640",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BPE Implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79ac68fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ï¼µï½Žï½‰ï½ƒï½ï½„ï½…! ðŸ…¤ðŸ…ðŸ…˜ðŸ…’ðŸ…žðŸ…“ðŸ…”â€½ ðŸ‡ºâ€ŒðŸ‡³â€ŒðŸ‡®â€ŒðŸ‡¨â€ŒðŸ‡´â€ŒðŸ‡©â€ŒðŸ‡ª! ðŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to \"support Unicode\" in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don't blame programmers for still finding the whole thing mysterious, even 30 years after Unicode's inception.\n",
      "Length in characters: 533\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get the sample text from Nathan Reed's blog post\n",
    "text = \"\"\"ï¼µï½Žï½‰ï½ƒï½ï½„ï½…! ðŸ…¤ðŸ…ðŸ…˜ðŸ…’ðŸ…žðŸ…“ðŸ…”â€½ ðŸ‡ºâ€ŒðŸ‡³â€ŒðŸ‡®â€ŒðŸ‡¨â€ŒðŸ‡´â€ŒðŸ‡©â€ŒðŸ‡ª! ðŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to \"support Unicode\" in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don't blame programmers for still finding the whole thing mysterious, even 30 years after Unicode's inception.\"\"\"\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Length in characters: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c3fd650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï¼µ\n",
      "b'\\xef\\xbc\\xb5'\n",
      "[239, 188, 181]\n"
     ]
    }
   ],
   "source": [
    "x = text[:1]\n",
    "print(x)\n",
    "print(x.encode(\"utf-8\"))\n",
    "print(list(x.encode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45e91223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTF-8 Tokens: [239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 34, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 34, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 39, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 39, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "Number of UTF-8 Tokens: 608\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Encode the text as UTF-8 and display the byte tokens\n",
    "tokens = list(text.encode(\"utf-8\"))\n",
    "\n",
    "print(f\"UTF-8 Tokens: {tokens}\")\n",
    "print(f\"Number of UTF-8 Tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "779b1366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids: list[int], counts=None) -> dict[tuple[int, int], int]:\n",
    "\n",
    "    counts = counts or {}\n",
    "    for pairs in list(zip(ids, ids[1:])):\n",
    "        counts[pairs] = counts.get(pairs, 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2756f8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample list: [1, 2, 3, 4, 5]\n",
      "Consecutive pairs: [(1, 2), (2, 3), (3, 4), (4, 5)]\n",
      "This is the 'Pythonic way' Andrej mentions for iterating consecutive elements\n"
     ]
    }
   ],
   "source": [
    "# Step 3a: Understand how zip(ids, ids[1:]) works for consecutive pairs\n",
    "sample_list = [1, 2, 3, 4, 5]\n",
    "consecutive_pairs = list(zip(sample_list, sample_list[1:]))\n",
    "print(f\"Sample list: {sample_list}\")\n",
    "print(f\"Consecutive pairs: {consecutive_pairs}\")\n",
    "print(\"This is the 'Pythonic way' Andrej mentions for iterating consecutive elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66446c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 pairs:\n",
      "Pair: (101, 32), Count: 20\n",
      "Pair: (240, 159), Count: 15\n",
      "Pair: (105, 110), Count: 12\n",
      "Pair: (115, 32), Count: 10\n",
      "Pair: (97, 110), Count: 10\n",
      "Pair: (32, 97), Count: 10\n",
      "Pair: (32, 116), Count: 9\n",
      "Pair: (226, 128), Count: 8\n",
      "Pair: (116, 104), Count: 8\n",
      "Pair: (159, 135), Count: 7\n"
     ]
    }
   ],
   "source": [
    "stats = get_stats(tokens)\n",
    "\n",
    "top_pairs = sorted(((count, pair) for pair, count in stats.items()), reverse=True)[:10]\n",
    "print(\"Top 10 pairs:\")\n",
    "for count, pair in top_pairs:\n",
    "    print(f\"Pair: {pair}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47432e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent pair: (101, 32)\n",
      "Occurs 20 times\n",
      "This represents: 'e' + ' '\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Get the most frequent pair using max() function\n",
    "most_frequent_pair = max(stats, key=stats.get)\n",
    "print(f\"Most frequent pair: {most_frequent_pair}\")\n",
    "print(f\"Occurs {stats[most_frequent_pair]} times\")\n",
    "\n",
    "# Convert bytes back to characters to see what this pair represents\n",
    "char1 = chr(most_frequent_pair[0])\n",
    "char2 = chr(most_frequent_pair[1])\n",
    "print(f\"This represents: '{char1}' + '{char2}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6acf9bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 occurrences of pair (101, 32) ('e' + ' ') at positions:\n",
      "Positions: [110, 120, 141, 150, 186, 198, 241, 269, 295, 325, 332, 362, 376, 384, 461, 466, 480, 508, 542, 548]\n"
     ]
    }
   ],
   "source": [
    "# Step 4a: Verify the most frequent pair by finding its occurrences in the text\n",
    "pair_to_find = most_frequent_pair  # (101, 32) which is 'e' + ' '\n",
    "\n",
    "# Find all positions where this pair occurs\n",
    "occurrences = []\n",
    "for i in range(len(tokens) - 1):\n",
    "    if tokens[i] == pair_to_find[0] and tokens[i + 1] == pair_to_find[1]:\n",
    "        occurrences.append(i)\n",
    "\n",
    "print(f\"Found {len(occurrences)} occurrences of pair {pair_to_find} ('e' + ' ') at positions:\")\n",
    "print(f\"Positions: {occurrences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "75d46d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will replace pair (101, 32) with new token ID: 256\n",
      "Ready to implement merge function...\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Prepare to merge - create new token ID\n",
    "# Current tokens are 0-255 (256 possible values), so new token will be 256\n",
    "new_token_id = 256\n",
    "print(f\"Will replace pair {most_frequent_pair} with new token ID: {new_token_id}\")\n",
    "print(f\"Ready to implement merge function...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "03d5b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    \n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n",
    "            new_ids.append(idx)\n",
    "            i += 2 # skipping the pair\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b90aaa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [5, 6, 6, 7, 9, 1]\n",
      "After merging (6, 7) -> 99: [5, 6, 99, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "# testing function merge\n",
    "\n",
    "test_ids = [5, 6, 6, 7, 9, 1]\n",
    "result = merge(test_ids, (6,7), 99)\n",
    "\n",
    "assert result == [5, 6, 99, 9, 1]\n",
    "print(f\"Original: {test_ids}\")\n",
    "print(f\"After merging (6, 7) -> 99: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "098a1a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length: 608\n",
      "After merge length: 588\n",
      "Reduction: 20 tokens\n",
      "\n",
      "Occurrences of new token 256: 20\n",
      "Occurrences of old pair in original: 20\n",
      "Occurrences of old pair in new tokens: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Apply merge to our actual tokens\n",
    "# Merge the most frequent pair (101, 32) with token ID 256\n",
    "tokens2 = merge(tokens, most_frequent_pair, new_token_id)\n",
    "\n",
    "print(f\"Original length: {len(tokens)}\")\n",
    "print(f\"After merge length: {len(tokens2)}\")\n",
    "print(f\"Reduction: {len(tokens) - len(tokens2)} tokens\")\n",
    "\n",
    "# Verify the merge worked\n",
    "print(f\"\\nOccurrences of new token {new_token_id}: {tokens2.count(new_token_id)}\")\n",
    "print(f\"Occurrences of old pair in original: {sum(1 for i in range(len(tokens)-1) if (tokens[i], tokens[i+1]) == most_frequent_pair)}\")\n",
    "\n",
    "# Verify old pair is gone\n",
    "old_pair_count = sum(1 for i in range(len(tokens2)-1) if (tokens2[i], tokens2[i+1]) == most_frequent_pair)\n",
    "print(f\"Occurrences of old pair in new tokens: {old_pair_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5bec5f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Training Progress:\n",
      "Step 0: 608 tokens, vocab size: 256\n",
      "Step 1: 588 tokens, vocab size: 257\n",
      "Step 2: 573 tokens, vocab size: 258\n",
      "  Merged pair: (240, 159) -> 257\n",
      "Step 3: 561 tokens, vocab size: 259\n",
      "  Merged pair: (105, 110) -> 258\n",
      "Step 4: 551 tokens, vocab size: 260\n",
      "  Merged pair: (115, 32) -> 259\n",
      "Step 5: 541 tokens, vocab size: 261\n",
      "  Merged pair: (97, 110) -> 260\n",
      "\n",
      "Final: 541 tokens, vocab size: 261\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Iterate the BPE algorithm\n",
    "# Now we repeat: find most common pair, merge it, repeat...\n",
    "# Let's do a few more iterations\n",
    "\n",
    "current_tokens = tokens2\n",
    "vocab_size = 257  # Started with 256, now have 257\n",
    "\n",
    "print(\"BPE Training Progress:\")\n",
    "print(f\"Step 0: {len(tokens)} tokens, vocab size: 256\")\n",
    "print(f\"Step 1: {len(current_tokens)} tokens, vocab size: {vocab_size}\")\n",
    "\n",
    "# Do a few more iterations\n",
    "for step in range(2, 6):  # Steps 2-5\n",
    "    # Find most common pair\n",
    "    stats = get_stats(current_tokens)\n",
    "    if not stats:  # No more pairs to merge\n",
    "        break\n",
    "    \n",
    "    most_frequent_pair = max(stats, key=stats.get)\n",
    "    \n",
    "    # Merge it\n",
    "    current_tokens = merge(current_tokens, most_frequent_pair, vocab_size)\n",
    "    \n",
    "    print(f\"Step {step}: {len(current_tokens)} tokens, vocab size: {vocab_size + 1}\")\n",
    "    print(f\"  Merged pair: {most_frequent_pair} -> {vocab_size}\")\n",
    "    \n",
    "    vocab_size += 1\n",
    "\n",
    "print(f\"\\nFinal: {len(current_tokens)} tokens, vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4ae0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 256: (101, 32) -> 'e' + ' ' = 'e '\n",
      "Token 257: (100, 32) -> 'd' + ' ' = 'd '\n",
      "Token 258: (116, 101) -> 't' + 'e' = 'te'\n",
      "Token 259: (115, 32) -> 's' + ' ' = 's '\n",
      "Token 260: (105, 110) -> 'i' + 'n' = 'in'\n"
     ]
    }
   ],
   "source": [
    "# Track the merges we made\n",
    "merges = {\n",
    "    256: (101, 32),  # 'e' + ' '\n",
    "    257: (100, 32),  # 'd' + ' '  \n",
    "    258: (116, 101), # 't' + 'e'\n",
    "    259: (115, 32),  # 's' + ' '\n",
    "    260: (105, 110)  # 'i' + 'n'\n",
    "}\n",
    "\n",
    "for token_id, (byte1, byte2) in merges.items():\n",
    "    char1, char2 = chr(byte1), chr(byte2)\n",
    "    print(f\"Token {token_id}: ({byte1}, {byte2}) -> '{char1}' + '{char2}' = '{char1}{char2}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5380e3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge 1/20: (101, 32) -> 256 (20 occurrences)\n",
      "merge 2/20: (240, 159) -> 257 (15 occurrences)\n",
      "merge 3/20: (105, 110) -> 258 (12 occurrences)\n",
      "merge 4/20: (115, 32) -> 259 (10 occurrences)\n",
      "merge 5/20: (97, 110) -> 260 (10 occurrences)\n",
      "merge 6/20: (226, 128) -> 261 (8 occurrences)\n",
      "merge 7/20: (116, 104) -> 262 (8 occurrences)\n",
      "merge 8/20: (257, 133) -> 263 (7 occurrences)\n",
      "merge 9/20: (257, 135) -> 264 (7 occurrences)\n",
      "merge 10/20: (97, 114) -> 265 (7 occurrences)\n",
      "merge 11/20: (239, 189) -> 266 (6 occurrences)\n",
      "merge 12/20: (261, 140) -> 267 (6 occurrences)\n",
      "merge 13/20: (267, 264) -> 268 (6 occurrences)\n",
      "merge 14/20: (101, 114) -> 269 (6 occurrences)\n",
      "merge 15/20: (111, 114) -> 270 (6 occurrences)\n",
      "merge 16/20: (116, 32) -> 271 (6 occurrences)\n",
      "merge 17/20: (258, 103) -> 272 (6 occurrences)\n",
      "merge 18/20: (115, 116) -> 273 (5 occurrences)\n",
      "merge 19/20: (260, 100) -> 274 (5 occurrences)\n",
      "merge 20/20: (32, 262) -> 275 (5 occurrences)\n"
     ]
    }
   ],
   "source": [
    "# BPE training\n",
    "vocab_size = 276  # hyperparameter: the desired final vocabulary size\n",
    "num_merges = vocab_size - 256\n",
    "tokens = list(text.encode(\"utf-8\"))\n",
    "\n",
    "for i in range(num_merges):\n",
    "    # count up all the pairs\n",
    "    stats = get_stats(tokens)\n",
    "    # find the pair with the highest count\n",
    "    pair = max(stats, key=stats.get)\n",
    "    # mint a new token: assign it the next available id\n",
    "    idx = 256 + i\n",
    "    # replace all occurrences of pair in tokens with idx\n",
    "    tokens = merge(tokens, pair, idx)\n",
    "    # print progress\n",
    "    print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({stats[pair]} occurrences)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0247fbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 32 ->  \n",
      "Token 33 -> !\n",
      "Token 33 -> !\n",
      "Token 34 -> \"\n",
      "Token 34 -> \"\n",
      "Token 39 -> '\n",
      "Token 39 -> '\n",
      "Token 40 -> (\n",
      "Token 41 -> )\n",
      "Token 44 -> ,\n",
      "Token 44 -> ,\n",
      "Token 44 -> ,\n",
      "Token 44 -> ,\n",
      "Token 44 -> ,\n",
      "Token 45 -> -\n",
      "Token 46 -> .\n",
      "Token 46 -> .\n",
      "Token 46 -> .\n",
      "Token 46 -> .\n",
      "Token 48 -> 0\n",
      "Token 51 -> 3\n",
      "Token 63 -> ?\n",
      "Token 66 -> B\n",
      "Token 73 -> I\n",
      "Token 83 -> S\n",
      "Token 84 -> T\n",
      "Token 85 -> U\n",
      "Token 85 -> U\n",
      "Token 85 -> U\n",
      "Token 85 -> U\n",
      "Token 87 -> W\n",
      "Token 95 -> _\n",
      "Token 97 -> a\n",
      "Token 97 -> a\n",
      "Token 97 -> a\n",
      "Token 97 -> a\n",
      "Token 97 -> a\n",
      "Token 97 -> a\n",
      "Token 97 -> a\n",
      "Token 97 -> a\n",
      "Token 97 -> a\n",
      "Token 97 -> a\n",
      "Token 97 -> a\n",
      "Token 97 -> a\n",
      "Token 97 -> a\n",
      "Token 97 -> a\n",
      "Token 98 -> b\n",
      "Token 98 -> b\n",
      "Token 98 -> b\n",
      "Token 98 -> b\n",
      "Token 99 -> c\n",
      "Token 99 -> c\n",
      "Token 99 -> c\n",
      "Token 99 -> c\n",
      "Token 99 -> c\n",
      "Token 99 -> c\n",
      "Token 99 -> c\n",
      "Token 99 -> c\n",
      "Token 100 -> d\n",
      "Token 100 -> d\n",
      "Token 100 -> d\n",
      "Token 100 -> d\n",
      "Token 100 -> d\n",
      "Token 100 -> d\n",
      "Token 100 -> d\n",
      "Token 100 -> d\n",
      "Token 100 -> d\n",
      "Token 100 -> d\n",
      "Token 100 -> d\n",
      "Token 100 -> d\n",
      "Token 101 -> e\n",
      "Token 101 -> e\n",
      "Token 101 -> e\n",
      "Token 101 -> e\n",
      "Token 101 -> e\n",
      "Token 101 -> e\n",
      "Token 101 -> e\n",
      "Token 101 -> e\n",
      "Token 101 -> e\n",
      "Token 101 -> e\n",
      "Token 101 -> e\n",
      "Token 101 -> e\n",
      "Token 101 -> e\n",
      "Token 101 -> e\n",
      "Token 101 -> e\n",
      "Token 101 -> e\n",
      "Token 101 -> e\n",
      "Token 101 -> e\n",
      "Token 101 -> e\n",
      "Token 101 -> e\n",
      "Token 102 -> f\n",
      "Token 102 -> f\n",
      "Token 102 -> f\n",
      "Token 102 -> f\n",
      "Token 102 -> f\n",
      "Token 102 -> f\n",
      "Token 102 -> f\n",
      "Token 102 -> f\n",
      "Token 103 -> g\n",
      "Token 103 -> g\n",
      "Token 103 -> g\n",
      "Token 103 -> g\n",
      "Token 103 -> g\n",
      "Token 104 -> h\n",
      "Token 104 -> h\n",
      "Token 104 -> h\n",
      "Token 104 -> h\n",
      "Token 104 -> h\n",
      "Token 104 -> h\n",
      "Token 104 -> h\n",
      "Token 105 -> i\n",
      "Token 105 -> i\n",
      "Token 105 -> i\n",
      "Token 105 -> i\n",
      "Token 105 -> i\n",
      "Token 105 -> i\n",
      "Token 105 -> i\n",
      "Token 105 -> i\n",
      "Token 105 -> i\n",
      "Token 105 -> i\n",
      "Token 105 -> i\n",
      "Token 105 -> i\n",
      "Token 105 -> i\n",
      "Token 105 -> i\n",
      "Token 105 -> i\n",
      "Token 105 -> i\n",
      "Token 107 -> k\n",
      "Token 107 -> k\n",
      "Token 107 -> k\n",
      "Token 108 -> l\n",
      "Token 108 -> l\n",
      "Token 108 -> l\n",
      "Token 108 -> l\n",
      "Token 108 -> l\n",
      "Token 108 -> l\n",
      "Token 108 -> l\n",
      "Token 108 -> l\n",
      "Token 108 -> l\n",
      "Token 108 -> l\n",
      "Token 108 -> l\n",
      "Token 108 -> l\n",
      "Token 108 -> l\n",
      "Token 108 -> l\n",
      "Token 109 -> m\n",
      "Token 109 -> m\n",
      "Token 109 -> m\n",
      "Token 109 -> m\n",
      "Token 109 -> m\n",
      "Token 109 -> m\n",
      "Token 109 -> m\n",
      "Token 109 -> m\n",
      "Token 109 -> m\n",
      "Token 109 -> m\n",
      "Token 109 -> m\n",
      "Token 110 -> n\n",
      "Token 110 -> n\n",
      "Token 110 -> n\n",
      "Token 110 -> n\n",
      "Token 110 -> n\n",
      "Token 110 -> n\n",
      "Token 110 -> n\n",
      "Token 110 -> n\n",
      "Token 110 -> n\n",
      "Token 110 -> n\n",
      "Token 110 -> n\n",
      "Token 110 -> n\n",
      "Token 110 -> n\n",
      "Token 111 -> o\n",
      "Token 111 -> o\n",
      "Token 111 -> o\n",
      "Token 111 -> o\n",
      "Token 111 -> o\n",
      "Token 111 -> o\n",
      "Token 111 -> o\n",
      "Token 111 -> o\n",
      "Token 111 -> o\n",
      "Token 111 -> o\n",
      "Token 111 -> o\n",
      "Token 111 -> o\n",
      "Token 111 -> o\n",
      "Token 111 -> o\n",
      "Token 111 -> o\n",
      "Token 111 -> o\n",
      "Token 111 -> o\n",
      "Token 111 -> o\n",
      "Token 111 -> o\n",
      "Token 111 -> o\n",
      "Token 111 -> o\n",
      "Token 111 -> o\n",
      "Token 112 -> p\n",
      "Token 112 -> p\n",
      "Token 112 -> p\n",
      "Token 112 -> p\n",
      "Token 112 -> p\n",
      "Token 112 -> p\n",
      "Token 112 -> p\n",
      "Token 112 -> p\n",
      "Token 112 -> p\n",
      "Token 112 -> p\n",
      "Token 114 -> r\n",
      "Token 114 -> r\n",
      "Token 114 -> r\n",
      "Token 114 -> r\n",
      "Token 114 -> r\n",
      "Token 114 -> r\n",
      "Token 114 -> r\n",
      "Token 114 -> r\n",
      "Token 114 -> r\n",
      "Token 114 -> r\n",
      "Token 115 -> s\n",
      "Token 115 -> s\n",
      "Token 115 -> s\n",
      "Token 115 -> s\n",
      "Token 115 -> s\n",
      "Token 115 -> s\n",
      "Token 115 -> s\n",
      "Token 115 -> s\n",
      "Token 115 -> s\n",
      "Token 115 -> s\n",
      "Token 115 -> s\n",
      "Token 116 -> t\n",
      "Token 116 -> t\n",
      "Token 116 -> t\n",
      "Token 116 -> t\n",
      "Token 116 -> t\n",
      "Token 116 -> t\n",
      "Token 116 -> t\n",
      "Token 116 -> t\n",
      "Token 116 -> t\n",
      "Token 116 -> t\n",
      "Token 116 -> t\n",
      "Token 116 -> t\n",
      "Token 116 -> t\n",
      "Token 116 -> t\n",
      "Token 116 -> t\n",
      "Token 116 -> t\n",
      "Token 116 -> t\n",
      "Token 116 -> t\n",
      "Token 117 -> u\n",
      "Token 117 -> u\n",
      "Token 117 -> u\n",
      "Token 117 -> u\n",
      "Token 117 -> u\n",
      "Token 117 -> u\n",
      "Token 117 -> u\n",
      "Token 117 -> u\n",
      "Token 117 -> u\n",
      "Token 117 -> u\n",
      "Token 118 -> v\n",
      "Token 118 -> v\n",
      "Token 118 -> v\n",
      "Token 118 -> v\n",
      "Token 119 -> w\n",
      "Token 119 -> w\n",
      "Token 119 -> w\n",
      "Token 119 -> w\n",
      "Token 119 -> w\n",
      "Token 119 -> w\n",
      "Token 119 -> w\n",
      "Token 119 -> w\n",
      "Token 119 -> w\n",
      "Token 120 -> x\n",
      "Token 121 -> y\n",
      "Token 121 -> y\n",
      "Token 121 -> y\n",
      "Token 121 -> y\n",
      "Token 122 -> z\n",
      "Token 131 -> Âƒ\n",
      "Token 132 -> Â„\n",
      "Token 132 -> Â„\n",
      "Token 133 -> Â…\n",
      "Token 137 -> Â‰\n",
      "Token 142 -> ÂŽ\n",
      "Token 143 -> Â\n",
      "Token 146 -> Â’\n",
      "Token 147 -> Â“\n",
      "Token 148 -> Â”\n",
      "Token 148 -> Â”\n",
      "Token 152 -> Â˜\n",
      "Token 152 -> Â˜\n",
      "Token 157 -> Â\n",
      "Token 158 -> Âž\n",
      "Token 164 -> Â¤\n",
      "Token 168 -> Â¨\n",
      "Token 169 -> Â©\n",
      "Token 170 -> Âª\n",
      "Token 174 -> Â®\n",
      "Token 179 -> Â³\n",
      "Token 180 -> Â´\n",
      "Token 181 -> Âµ\n",
      "Token 186 -> Âº\n",
      "Token 188 -> Â¼\n",
      "Token 189 -> Â½\n",
      "Token 239 -> Ã¯\n",
      "Token 256 -> Ä€\n",
      "Token 256 -> Ä€\n",
      "Token 256 -> Ä€\n",
      "Token 256 -> Ä€\n",
      "Token 256 -> Ä€\n",
      "Token 256 -> Ä€\n",
      "Token 256 -> Ä€\n",
      "Token 256 -> Ä€\n",
      "Token 256 -> Ä€\n",
      "Token 256 -> Ä€\n",
      "Token 256 -> Ä€\n",
      "Token 256 -> Ä€\n",
      "Token 256 -> Ä€\n",
      "Token 256 -> Ä€\n",
      "Token 256 -> Ä€\n",
      "Token 256 -> Ä€\n",
      "Token 256 -> Ä€\n",
      "Token 256 -> Ä€\n",
      "Token 256 -> Ä€\n",
      "Token 256 -> Ä€\n",
      "Token 257 -> Ä\n",
      "Token 258 -> Ä‚\n",
      "Token 258 -> Ä‚\n",
      "Token 258 -> Ä‚\n",
      "Token 258 -> Ä‚\n",
      "Token 258 -> Ä‚\n",
      "Token 258 -> Ä‚\n",
      "Token 259 -> Äƒ\n",
      "Token 259 -> Äƒ\n",
      "Token 259 -> Äƒ\n",
      "Token 259 -> Äƒ\n",
      "Token 259 -> Äƒ\n",
      "Token 259 -> Äƒ\n",
      "Token 259 -> Äƒ\n",
      "Token 259 -> Äƒ\n",
      "Token 259 -> Äƒ\n",
      "Token 259 -> Äƒ\n",
      "Token 260 -> Ä„\n",
      "Token 260 -> Ä„\n",
      "Token 260 -> Ä„\n",
      "Token 260 -> Ä„\n",
      "Token 260 -> Ä„\n",
      "Token 261 -> Ä…\n",
      "Token 261 -> Ä…\n",
      "Token 262 -> Ä†\n",
      "Token 262 -> Ä†\n",
      "Token 262 -> Ä†\n",
      "Token 263 -> Ä‡\n",
      "Token 263 -> Ä‡\n",
      "Token 263 -> Ä‡\n",
      "Token 263 -> Ä‡\n",
      "Token 263 -> Ä‡\n",
      "Token 263 -> Ä‡\n",
      "Token 263 -> Ä‡\n",
      "Token 264 -> Äˆ\n",
      "Token 265 -> Ä‰\n",
      "Token 265 -> Ä‰\n",
      "Token 265 -> Ä‰\n",
      "Token 265 -> Ä‰\n",
      "Token 265 -> Ä‰\n",
      "Token 265 -> Ä‰\n",
      "Token 265 -> Ä‰\n",
      "Token 266 -> ÄŠ\n",
      "Token 266 -> ÄŠ\n",
      "Token 266 -> ÄŠ\n",
      "Token 266 -> ÄŠ\n",
      "Token 266 -> ÄŠ\n",
      "Token 266 -> ÄŠ\n",
      "Token 268 -> ÄŒ\n",
      "Token 268 -> ÄŒ\n",
      "Token 268 -> ÄŒ\n",
      "Token 268 -> ÄŒ\n",
      "Token 268 -> ÄŒ\n",
      "Token 268 -> ÄŒ\n",
      "Token 269 -> Ä\n",
      "Token 269 -> Ä\n",
      "Token 269 -> Ä\n",
      "Token 269 -> Ä\n",
      "Token 269 -> Ä\n",
      "Token 269 -> Ä\n",
      "Token 270 -> ÄŽ\n",
      "Token 270 -> ÄŽ\n",
      "Token 270 -> ÄŽ\n",
      "Token 270 -> ÄŽ\n",
      "Token 270 -> ÄŽ\n",
      "Token 270 -> ÄŽ\n",
      "Token 271 -> Ä\n",
      "Token 271 -> Ä\n",
      "Token 271 -> Ä\n",
      "Token 271 -> Ä\n",
      "Token 271 -> Ä\n",
      "Token 271 -> Ä\n",
      "Token 272 -> Ä\n",
      "Token 272 -> Ä\n",
      "Token 272 -> Ä\n",
      "Token 272 -> Ä\n",
      "Token 272 -> Ä\n",
      "Token 272 -> Ä\n",
      "Token 273 -> Ä‘\n",
      "Token 273 -> Ä‘\n",
      "Token 273 -> Ä‘\n",
      "Token 273 -> Ä‘\n",
      "Token 273 -> Ä‘\n",
      "Token 274 -> Ä’\n",
      "Token 274 -> Ä’\n",
      "Token 274 -> Ä’\n",
      "Token 274 -> Ä’\n",
      "Token 274 -> Ä’\n",
      "Token 275 -> Ä“\n",
      "Token 275 -> Ä“\n",
      "Token 275 -> Ä“\n",
      "Token 275 -> Ä“\n",
      "Token 275 -> Ä“\n"
     ]
    }
   ],
   "source": [
    "for i in sorted(tokens):\n",
    "    # if i < 256:\n",
    "    print(f\"Token {i} -> {chr(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f45d33ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track the merges we made\n",
    "merges = {\n",
    "    (101, 32) : 256,  # 'e' + ' '\n",
    "    (100, 32) : 257,  # 'd' + ' '  \n",
    "    (116, 101) : 258, # 't' + 'e'\n",
    "    (115, 32) : 259,  # 's' + ' '\n",
    "    (105, 110): 260  # 'i' + 'n'\n",
    "}\n",
    "# given ids (list of integers), return Python string\n",
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "    # given ids, get tokens\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    # convert from bytes to string\n",
    "    text = tokens.decode(\"utf-8\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3b11b569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'\\x00',\n",
       " 1: b'\\x01',\n",
       " 2: b'\\x02',\n",
       " 3: b'\\x03',\n",
       " 4: b'\\x04',\n",
       " 5: b'\\x05',\n",
       " 6: b'\\x06',\n",
       " 7: b'\\x07',\n",
       " 8: b'\\x08',\n",
       " 9: b'\\t',\n",
       " 10: b'\\n',\n",
       " 11: b'\\x0b',\n",
       " 12: b'\\x0c',\n",
       " 13: b'\\r',\n",
       " 14: b'\\x0e',\n",
       " 15: b'\\x0f',\n",
       " 16: b'\\x10',\n",
       " 17: b'\\x11',\n",
       " 18: b'\\x12',\n",
       " 19: b'\\x13',\n",
       " 20: b'\\x14',\n",
       " 21: b'\\x15',\n",
       " 22: b'\\x16',\n",
       " 23: b'\\x17',\n",
       " 24: b'\\x18',\n",
       " 25: b'\\x19',\n",
       " 26: b'\\x1a',\n",
       " 27: b'\\x1b',\n",
       " 28: b'\\x1c',\n",
       " 29: b'\\x1d',\n",
       " 30: b'\\x1e',\n",
       " 31: b'\\x1f',\n",
       " 32: b' ',\n",
       " 33: b'!',\n",
       " 34: b'\"',\n",
       " 35: b'#',\n",
       " 36: b'$',\n",
       " 37: b'%',\n",
       " 38: b'&',\n",
       " 39: b\"'\",\n",
       " 40: b'(',\n",
       " 41: b')',\n",
       " 42: b'*',\n",
       " 43: b'+',\n",
       " 44: b',',\n",
       " 45: b'-',\n",
       " 46: b'.',\n",
       " 47: b'/',\n",
       " 48: b'0',\n",
       " 49: b'1',\n",
       " 50: b'2',\n",
       " 51: b'3',\n",
       " 52: b'4',\n",
       " 53: b'5',\n",
       " 54: b'6',\n",
       " 55: b'7',\n",
       " 56: b'8',\n",
       " 57: b'9',\n",
       " 58: b':',\n",
       " 59: b';',\n",
       " 60: b'<',\n",
       " 61: b'=',\n",
       " 62: b'>',\n",
       " 63: b'?',\n",
       " 64: b'@',\n",
       " 65: b'A',\n",
       " 66: b'B',\n",
       " 67: b'C',\n",
       " 68: b'D',\n",
       " 69: b'E',\n",
       " 70: b'F',\n",
       " 71: b'G',\n",
       " 72: b'H',\n",
       " 73: b'I',\n",
       " 74: b'J',\n",
       " 75: b'K',\n",
       " 76: b'L',\n",
       " 77: b'M',\n",
       " 78: b'N',\n",
       " 79: b'O',\n",
       " 80: b'P',\n",
       " 81: b'Q',\n",
       " 82: b'R',\n",
       " 83: b'S',\n",
       " 84: b'T',\n",
       " 85: b'U',\n",
       " 86: b'V',\n",
       " 87: b'W',\n",
       " 88: b'X',\n",
       " 89: b'Y',\n",
       " 90: b'Z',\n",
       " 91: b'[',\n",
       " 92: b'\\\\',\n",
       " 93: b']',\n",
       " 94: b'^',\n",
       " 95: b'_',\n",
       " 96: b'`',\n",
       " 97: b'a',\n",
       " 98: b'b',\n",
       " 99: b'c',\n",
       " 100: b'd',\n",
       " 101: b'e',\n",
       " 102: b'f',\n",
       " 103: b'g',\n",
       " 104: b'h',\n",
       " 105: b'i',\n",
       " 106: b'j',\n",
       " 107: b'k',\n",
       " 108: b'l',\n",
       " 109: b'm',\n",
       " 110: b'n',\n",
       " 111: b'o',\n",
       " 112: b'p',\n",
       " 113: b'q',\n",
       " 114: b'r',\n",
       " 115: b's',\n",
       " 116: b't',\n",
       " 117: b'u',\n",
       " 118: b'v',\n",
       " 119: b'w',\n",
       " 120: b'x',\n",
       " 121: b'y',\n",
       " 122: b'z',\n",
       " 123: b'{',\n",
       " 124: b'|',\n",
       " 125: b'}',\n",
       " 126: b'~',\n",
       " 127: b'\\x7f',\n",
       " 128: b'\\x80',\n",
       " 129: b'\\x81',\n",
       " 130: b'\\x82',\n",
       " 131: b'\\x83',\n",
       " 132: b'\\x84',\n",
       " 133: b'\\x85',\n",
       " 134: b'\\x86',\n",
       " 135: b'\\x87',\n",
       " 136: b'\\x88',\n",
       " 137: b'\\x89',\n",
       " 138: b'\\x8a',\n",
       " 139: b'\\x8b',\n",
       " 140: b'\\x8c',\n",
       " 141: b'\\x8d',\n",
       " 142: b'\\x8e',\n",
       " 143: b'\\x8f',\n",
       " 144: b'\\x90',\n",
       " 145: b'\\x91',\n",
       " 146: b'\\x92',\n",
       " 147: b'\\x93',\n",
       " 148: b'\\x94',\n",
       " 149: b'\\x95',\n",
       " 150: b'\\x96',\n",
       " 151: b'\\x97',\n",
       " 152: b'\\x98',\n",
       " 153: b'\\x99',\n",
       " 154: b'\\x9a',\n",
       " 155: b'\\x9b',\n",
       " 156: b'\\x9c',\n",
       " 157: b'\\x9d',\n",
       " 158: b'\\x9e',\n",
       " 159: b'\\x9f',\n",
       " 160: b'\\xa0',\n",
       " 161: b'\\xa1',\n",
       " 162: b'\\xa2',\n",
       " 163: b'\\xa3',\n",
       " 164: b'\\xa4',\n",
       " 165: b'\\xa5',\n",
       " 166: b'\\xa6',\n",
       " 167: b'\\xa7',\n",
       " 168: b'\\xa8',\n",
       " 169: b'\\xa9',\n",
       " 170: b'\\xaa',\n",
       " 171: b'\\xab',\n",
       " 172: b'\\xac',\n",
       " 173: b'\\xad',\n",
       " 174: b'\\xae',\n",
       " 175: b'\\xaf',\n",
       " 176: b'\\xb0',\n",
       " 177: b'\\xb1',\n",
       " 178: b'\\xb2',\n",
       " 179: b'\\xb3',\n",
       " 180: b'\\xb4',\n",
       " 181: b'\\xb5',\n",
       " 182: b'\\xb6',\n",
       " 183: b'\\xb7',\n",
       " 184: b'\\xb8',\n",
       " 185: b'\\xb9',\n",
       " 186: b'\\xba',\n",
       " 187: b'\\xbb',\n",
       " 188: b'\\xbc',\n",
       " 189: b'\\xbd',\n",
       " 190: b'\\xbe',\n",
       " 191: b'\\xbf',\n",
       " 192: b'\\xc0',\n",
       " 193: b'\\xc1',\n",
       " 194: b'\\xc2',\n",
       " 195: b'\\xc3',\n",
       " 196: b'\\xc4',\n",
       " 197: b'\\xc5',\n",
       " 198: b'\\xc6',\n",
       " 199: b'\\xc7',\n",
       " 200: b'\\xc8',\n",
       " 201: b'\\xc9',\n",
       " 202: b'\\xca',\n",
       " 203: b'\\xcb',\n",
       " 204: b'\\xcc',\n",
       " 205: b'\\xcd',\n",
       " 206: b'\\xce',\n",
       " 207: b'\\xcf',\n",
       " 208: b'\\xd0',\n",
       " 209: b'\\xd1',\n",
       " 210: b'\\xd2',\n",
       " 211: b'\\xd3',\n",
       " 212: b'\\xd4',\n",
       " 213: b'\\xd5',\n",
       " 214: b'\\xd6',\n",
       " 215: b'\\xd7',\n",
       " 216: b'\\xd8',\n",
       " 217: b'\\xd9',\n",
       " 218: b'\\xda',\n",
       " 219: b'\\xdb',\n",
       " 220: b'\\xdc',\n",
       " 221: b'\\xdd',\n",
       " 222: b'\\xde',\n",
       " 223: b'\\xdf',\n",
       " 224: b'\\xe0',\n",
       " 225: b'\\xe1',\n",
       " 226: b'\\xe2',\n",
       " 227: b'\\xe3',\n",
       " 228: b'\\xe4',\n",
       " 229: b'\\xe5',\n",
       " 230: b'\\xe6',\n",
       " 231: b'\\xe7',\n",
       " 232: b'\\xe8',\n",
       " 233: b'\\xe9',\n",
       " 234: b'\\xea',\n",
       " 235: b'\\xeb',\n",
       " 236: b'\\xec',\n",
       " 237: b'\\xed',\n",
       " 238: b'\\xee',\n",
       " 239: b'\\xef',\n",
       " 240: b'\\xf0',\n",
       " 241: b'\\xf1',\n",
       " 242: b'\\xf2',\n",
       " 243: b'\\xf3',\n",
       " 244: b'\\xf4',\n",
       " 245: b'\\xf5',\n",
       " 246: b'\\xf6',\n",
       " 247: b'\\xf7',\n",
       " 248: b'\\xf8',\n",
       " 249: b'\\xf9',\n",
       " 250: b'\\xfa',\n",
       " 251: b'\\xfb',\n",
       " 252: b'\\xfc',\n",
       " 253: b'\\xfd',\n",
       " 254: b'\\xfe',\n",
       " 255: b'\\xff',\n",
       " 256: b'e ',\n",
       " 257: b'd ',\n",
       " 258: b'te',\n",
       " 259: b's ',\n",
       " 260: b'in'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6c3eee31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print(decode([97]))  # Should work fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab2a0a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
